[DATA]
dataset_name = gsm8k
config_name = main
split = train
split_ratio = 0.9

[TOKENIZER]
tokenizer_name = gpt2  # You can also use 'tiktoken' or other compatible tokenizers

[MODEL]
d_model = 512
n_heads = 8
n_layers = 6
max_seq_len = 256
vocab_size = 50257  # Matches the GPT-2 tokenizer vocab size

[TRAINING]
num_steps = 10000
print_every = 100
batch_size = 16
seq_len = 256
learning_rate = 0.0003

[CHECKPOINT]
checkpoint_dir = checkpoints
checkpoint_interval = 100
